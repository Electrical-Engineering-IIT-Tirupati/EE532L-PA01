{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can import libraries as per your need\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################## Cannot be modified ##############################################################\n",
    "# Logistic Regression using sklearn\n",
    "def regress_fit_sklearn(X_train, y_train, X_test):\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    poly = PolynomialFeatures(degree=3)\n",
    "    X_train_scaled = poly.fit_transform(X_train_scaled)\n",
    "    X_test_scaled = poly.transform(X_test_scaled)\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000, C=0.5)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    return y_test_pred\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"diabetes.csv\")\n",
    "X = df.drop(\"Outcome\", axis=1)\n",
    "y = df[\"Outcome\"]\n",
    "X_train = X[:614]\n",
    "X_test = X[614:]\n",
    "y_train = y[:614]\n",
    "y_test = y[614:]\n",
    "###########################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for the logistic regression below so that it returns y_test_pred\n",
    "# Q1) Normalize the training/testing features (Hint:- There are 8 features, so each of \n",
    "# them should be divided by the largest value of that specific feature ,among all the \n",
    "# samples)\n",
    "# (Note:- Take care of the axis or else accuracy won't increase)\n",
    "\n",
    "p = X_train.shape[0] # Number of features\n",
    "N = X_train.shape[1] # Number of sample cases\n",
    "\n",
    "# Q2) Set value of learning rate e\n",
    "e =  # Learning Rate (Hint:- adjust between 1e-5 to 1e-2)\n",
    "\n",
    "w = np.random.uniform(-1/np.sqrt(p), 1/np.sqrt(p), (p+1,1)) # Random initialization of\n",
    "# weights\n",
    "X = np.ones((p+1,N)) # Adding an extra column of ones to adjust biases\n",
    "X[:p,:] = X_train\n",
    "\n",
    "\n",
    "# Q3) Set number of epochs\n",
    "num_epochs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs): # Loop for iterative process\n",
    "    J = 0 # Initializing loss\n",
    "    count = 0 # Initializing count of correct predictions\n",
    "    for i in range (N):\n",
    "        z = ((w.T)@X[:,i:i+1])[0,0] # Raw logits (W.T x X)    \n",
    "        \n",
    "        # Q4) Write equation of Sigmoid(z)\n",
    "        y =  # Sigmoid activation function\n",
    "        T = y_train[i] # Ground Truth\n",
    "        \n",
    "        # Q5) Write loss function after the minus sign\n",
    "        J = J- # Loss function\n",
    "        # (Note:- The loss function is written after J = J- because we are trying to find\n",
    "        # the average loss per epoch, so we need to sum it iteratively )\n",
    "        \n",
    "        # Q6) Write Derivative of J w.r.t z\n",
    "        k =  # Derivative of J w.r.t z (Chain rule, J w.r.t y multiplied by y w.r.t z )\n",
    "        dJ = k*X[:,i:i+1] # Final Derivative of J w.r.t w (dJ/dz multiplied by dz/dw)\n",
    "        \n",
    "        # Q7) Write formula of Gradient Descent\n",
    "        w =  # Gradient Descent\n",
    "        \n",
    "        if abs(y-T)<0.5:\n",
    "            count = count+1 # Counting the number of correct predictions\n",
    "            \n",
    "        # Q8) So the two lines of code above is a method to find TP (True positives).\n",
    "        # So similarly write a code for finding TN, FP, FN\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    train_loss = J/N\n",
    "    train_accuracy = 100*count/N\n",
    "    \n",
    "    # Q9) Find the precision, recall, specificity, F1 score and IoU \n",
    "    \n",
    "\n",
    "    batch_metrics = f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Train Accuracy: {train_accuracy:.4f} \"\n",
    "    sys.stdout.write('\\r' + batch_metrics)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10) store all the metrics you have found above w.r.t epoch and plot the loss vs epoch\n",
    "# curve, accuracy vs epoch curve and similarly every metric you have found vs epoch curve\n",
    "# separately\n",
    "\n",
    "\n",
    "\n",
    "# Testing\n",
    "print(\"\\n\")\n",
    "N2 = X_test.shape[1] # Number of test samples\n",
    "\n",
    "X2 = np.ones((p+1,N2)) # adding an additional columns of 1 to adjust biases\n",
    "X2[:p,:] = X_test\n",
    "\n",
    "z2 = w.T@X2 # test logit matrix\n",
    "y_pred = 1/(1+np.exp(-z2)) # Sigmoid activation function to convert into probabilities\n",
    "y_pred[y_pred>=0.5] = 1 # Thresholding\n",
    "y_pred[y_pred<0.5] = 0\n",
    "\n",
    "# Fit the model\n",
    "y_test_pred_sk = regress_fit_sklearn(X_train, y_train, X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "test_accuracy_sk = accuracy_score(y_test, y_test_pred_sk)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy using sklearn: {test_accuracy_sk:.5f}\")\n",
    "print(f\"Test Accuracy using your implementation: {test_accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################################\n",
    "# BONUS QUESTION (For people who opted out of the project, Everyone is allowed to attempt anyway)\n",
    "\n",
    "# Repeat this whole process for combinations of sigmoid/tanh activation function \n",
    "# and Binary Crossentropy/MSE loss function (3 more possible combinations) and compare their metric curves\n",
    "############################################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
